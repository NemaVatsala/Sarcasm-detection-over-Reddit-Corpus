{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "import time\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Model Training\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "#import folium\n",
    "from itertools import cycle, islice\n",
    "from pandas import options\n",
    "import warnings\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "#!pip install torchtext\n",
    "#import torchtext\n",
    "#from torchtext import data\n",
    "#from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_curve \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import string\n",
    "import os, re, csv, math, codecs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf  # we use both tensorflow and pytorch (pytorch for main part) , tensorflow for tokenizer\n",
    "torch.manual_seed(1024);\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\2517431560.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  train_df=pd.read_csv(\"C:/Users/VATSALA NEMA/Documents/SEM 7/NLP 2022/Project- Sarcasm detection in Soc med/Dataset/project1/project_training_data_with_class_labels.csv\" , error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv(\"C:/Users/VATSALA NEMA/Documents/SEM 7/NLP 2022/Project- Sarcasm detection in Soc med/Dataset/project1/project_training_data_with_class_labels.csv\" , error_bad_lines=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Class Labels</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808098</th>\n",
       "      <td>Arectarius</td>\n",
       "      <td>50% hp/def elgif; impiety orb; bourn jewel che...</td>\n",
       "      <td>Hate to break it to you; but Gloomy can't do a...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808099</th>\n",
       "      <td>dale1v</td>\n",
       "      <td>Rubicon Mango.</td>\n",
       "      <td>Hey Reddit; if you could only drink one flavor...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808100</th>\n",
       "      <td>Semyonov</td>\n",
       "      <td>Essentially GAP insurance right?</td>\n",
       "      <td>I'm sure you can pay for it. Most insurance pl...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808101</th>\n",
       "      <td>futalover99</td>\n",
       "      <td>Yeah; but sometimes that weird kid keeps tryin...</td>\n",
       "      <td>Ive been subscribed to that for a while; and a...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808102</th>\n",
       "      <td>SuperSpartacus</td>\n",
       "      <td>Petroleum doesn't really require a terroir lik...</td>\n",
       "      <td>It must be very different for oil then. I know...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808060 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID                                           Comments  \\\n",
       "0               ocxtitan                                   Central Illinois   \n",
       "1              LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2          throwitskrub8  But then again; you have to consider that all ...   \n",
       "3       fresherthanyouuu                                            ughhhhh   \n",
       "4              _kushagra                                I should've put the   \n",
       "...                  ...                                                ...   \n",
       "808098        Arectarius  50% hp/def elgif; impiety orb; bourn jewel che...   \n",
       "808099            dale1v                                     Rubicon Mango.   \n",
       "808100          Semyonov                   Essentially GAP insurance right?   \n",
       "808101       futalover99  Yeah; but sometimes that weird kid keeps tryin...   \n",
       "808102    SuperSpartacus  Petroleum doesn't really require a terroir lik...   \n",
       "\n",
       "                                          Parent Comments  Class Labels   \\\n",
       "0                               Jesus; where do you live?  non-sarcastic   \n",
       "1       Even The CNN Staff Is Sick Of The Wall-To-Wall...  non-sarcastic   \n",
       "2       agree to that part.It can also mean that gujra...  non-sarcastic   \n",
       "3       If a guy told you he doesn't use social media ...  non-sarcastic   \n",
       "4       No; it's just a programming bug. After all; th...      sarcastic   \n",
       "...                                                   ...            ...   \n",
       "808098  Hate to break it to you; but Gloomy can't do a...      sarcastic   \n",
       "808099  Hey Reddit; if you could only drink one flavor...  non-sarcastic   \n",
       "808100  I'm sure you can pay for it. Most insurance pl...  non-sarcastic   \n",
       "808101  Ive been subscribed to that for a while; and a...  non-sarcastic   \n",
       "808102  It must be very different for oil then. I know...  non-sarcastic   \n",
       "\n",
       "       Unnamed: 4  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "808098        NaN  \n",
       "808099        NaN  \n",
       "808100        NaN  \n",
       "808101        NaN  \n",
       "808102        NaN  \n",
       "\n",
       "[808060 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1 = train_df[train_df.isna().any(axis=1)]\n",
    "#df1.head()\n",
    "train_df.dropna(subset=['Comments', 'Parent Comments'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the null comments\n",
    "train_df.dropna(subset=['Comments'], inplace=True)\n",
    "train_df['Comments'] = train_df['Comments'].str.lower()\n",
    "train_df['Comments'] = train_df['Comments'].str.replace('[^\\w\\s]','', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stop = list(stopwords.words('english'))\n",
    "def lowercase(text): #convert to lower case\n",
    "  return text.lower()\n",
    "\n",
    "def remove_punct(text): #remove punctuations using regex\n",
    "  return re.sub('[^a-z]+',' ',text)\n",
    "\n",
    "def remove_stopwords(text):  #remove stopwords to aavoid overfitting\n",
    "  mylist = []\n",
    "  for i in text.split():\n",
    "    if i not in stop:\n",
    "      mylist.append(i)\n",
    "  return \" \".join(mylist)\n",
    "\n",
    "\n",
    "def cleanmepls(text): #fn to clean col\n",
    "  text = lowercase(text)\n",
    "  text = remove_punct(text)\n",
    "  text = remove_stopwords(text)\n",
    "  return text\n",
    "\n",
    "train_df['clean_Comments'] = train_df['Comments'].apply(cleanmepls)\n",
    "\n",
    "def buildDictionary(texts):\n",
    "    \"\"\"\n",
    "    Build the dictionary of words where key is the word and value is the index.\n",
    "    \"\"\"\n",
    "    hashmap = {}\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            hashmap[word] = hashmap.get(word, 0) + 1\n",
    "    return {w:i+1 for i, w in enumerate(list(hashmap.keys()))}\n",
    "\n",
    "def calculateMaxSeqLen(texts):\n",
    "    \"\"\"\n",
    "    Calculates the maximum sequence length found in the corpus\n",
    "    \"\"\"\n",
    "    max_len = float('-inf')\n",
    "    for text in texts:\n",
    "        if len(text.split()) > max_len:\n",
    "            max_len = len(text.split())\n",
    "            \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Class Labels</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>clean_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>central illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>central illinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>to think  cnn used to be the acronym synonymou...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>think cnn used acronym synonymous news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>but then again you have to consider that all h...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>consider husbands suspecting adultery dont sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ughhhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>i shouldve put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shouldve put</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   central illinois   \n",
       "1         LeChuckly  to think  cnn used to be the acronym synonymou...   \n",
       "2     throwitskrub8  but then again you have to consider that all h...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                 i shouldve put the   \n",
       "\n",
       "                                     Parent Comments  Class Labels   \\\n",
       "0                          Jesus; where do you live?  non-sarcastic   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...  non-sarcastic   \n",
       "2  agree to that part.It can also mean that gujra...  non-sarcastic   \n",
       "3  If a guy told you he doesn't use social media ...  non-sarcastic   \n",
       "4  No; it's just a programming bug. After all; th...      sarcastic   \n",
       "\n",
       "  Unnamed: 4                                     clean_Comments  \n",
       "0        NaN                                   central illinois  \n",
       "1        NaN             think cnn used acronym synonymous news  \n",
       "2        NaN  consider husbands suspecting adultery dont sim...  \n",
       "3        NaN                                            ughhhhh  \n",
       "4        NaN                                       shouldve put  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\2786444516.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df.rename(columns = {' Class Labels ':'label'}, inplace = True)\n",
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\2786444516.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df.rename(columns = {'clean_Comments':'Comments'}, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>central illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think cnn used acronym synonymous news</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consider husbands suspecting adultery dont sim...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shouldve put</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments          label\n",
       "0                                   central illinois  non-sarcastic\n",
       "1             think cnn used acronym synonymous news  non-sarcastic\n",
       "2  consider husbands suspecting adultery dont sim...  non-sarcastic\n",
       "3                                            ughhhhh  non-sarcastic\n",
       "4                                       shouldve put      sarcastic"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df=train_df.iloc[:,[5,3]]\n",
    "new_df.rename(columns = {' Class Labels ':'label'}, inplace = True)\n",
    "new_df.rename(columns = {'clean_Comments':'Comments'}, inplace = True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\2576053051.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['Comments'] = new_df['Comments'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "new_df['Comments'] = new_df['Comments'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDictionary(texts):\n",
    "    \"\"\"\n",
    "    Build the dictionary of words where key is the word and value is the index.\n",
    "    \"\"\"\n",
    "    hashmap = {}\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            hashmap[word] = hashmap.get(word, 0) + 1\n",
    "    return {w:i+1 for i, w in enumerate(list(hashmap.keys()))}\n",
    "\n",
    "def calculateMaxSeqLen(texts):\n",
    "    \"\"\"\n",
    "    Calculates the maximum sequence length found in the corpus\n",
    "    \"\"\"\n",
    "    max_len = float('-inf')\n",
    "    for text in texts:\n",
    "        if len(text.split()) > max_len:\n",
    "            max_len = len(text.split())\n",
    "            \n",
    "    return max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 168953\n",
      "Maximum sequence lenght: 2000\n"
     ]
    }
   ],
   "source": [
    "dictionary = buildDictionary(new_df[\"Comments\"])\n",
    "max_seq_len = calculateMaxSeqLen(new_df[\"Comments\"])\n",
    "\n",
    "print(f\"Number of words in dictionary: {len(dictionary)}\")\n",
    "print(f\"Maximum sequence lenght: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\3488329964.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['Comments'] = new_df.Comments.apply(lemmatize_text).copy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments without stopwords</th>\n",
       "      <th>clean_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think cnn use acronym synonymous news</td>\n",
       "      <td>1</td>\n",
       "      <td>think cnn use acronym synonymous news</td>\n",
       "      <td>think cnn use acronym synonymous news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consider husband suspect adultery dont simply ...</td>\n",
       "      <td>1</td>\n",
       "      <td>consider husband suspect adultery dont simply ...</td>\n",
       "      <td>consider husband suspect adultery dont simply ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>1</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>ughhhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shouldve put</td>\n",
       "      <td>0</td>\n",
       "      <td>shouldve put</td>\n",
       "      <td>shouldve put</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ill take traditional german bake goods five th...</td>\n",
       "      <td>1</td>\n",
       "      <td>ill take traditional german bake goods five th...</td>\n",
       "      <td>ill take traditional german bake goods five th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments label  \\\n",
       "1              think cnn use acronym synonymous news     1   \n",
       "2  consider husband suspect adultery dont simply ...     1   \n",
       "3                                            ughhhhh     1   \n",
       "4                                       shouldve put     0   \n",
       "5  ill take traditional german bake goods five th...     1   \n",
       "\n",
       "                          Comments without stopwords  \\\n",
       "1              think cnn use acronym synonymous news   \n",
       "2  consider husband suspect adultery dont simply ...   \n",
       "3                                            ughhhhh   \n",
       "4                                       shouldve put   \n",
       "5  ill take traditional german bake goods five th...   \n",
       "\n",
       "                                      clean_Comments  \n",
       "1              think cnn use acronym synonymous news  \n",
       "2  consider husband suspect adultery dont simply ...  \n",
       "3                                            ughhhhh  \n",
       "4                                       shouldve put  \n",
       "5  ill take traditional german bake goods five th...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "new_df['Comments'] = new_df.Comments.apply(lemmatize_text).copy()\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\1285678393.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['label'].replace(['non-sarcastic','sarcastic'],[1,0], inplace = True)\n"
     ]
    }
   ],
   "source": [
    "new_df['label'].replace(['non-sarcastic','sarcastic'],[1,0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>central illinois</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think cnn use acronym synonymous news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consider husband suspect adultery dont simply ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shouldve put</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments label\n",
       "0                                   central illinois     1\n",
       "1              think cnn use acronym synonymous news     1\n",
       "2  consider husband suspect adultery dont simply ...     1\n",
       "3                                            ughhhhh     1\n",
       "4                                       shouldve put     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\" )\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def lemmatized_words(doc):\n",
    "  return (WordNetLemmatizer().lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "      w_line = line.split()\n",
    "      curr_word = w_line[0]\n",
    "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype='float32')\n",
    "      \n",
    "  return word_to_vec_map\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n",
    "    \"\"\"\n",
    "     this function create the embedding matrix save in numpy array\n",
    "    :param word_index: a dictionary with word: index_value\n",
    "    :param embedding_dict: a dict with word embedding\n",
    "    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n",
    "    ## loop over all the words\n",
    "    for word, index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "len(set(list(itertools.chain.from_iterable((new_df[\"Comments\"].apply(lambda x: x.split()).values.tolist()))))) #should be 170065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm>=4.40.0 in c:\\apps\\conda\\envs\\myenv\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\apps\\conda\\envs\\myenv\\lib\\site-packages (from tqdm>=4.40.0) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tqdm>=4.40.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "      w_line = line.split()\n",
    "      curr_word = w_line[0]\n",
    "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "  return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vector('./glove.twitter.27B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glove embedding\n"
     ]
    }
   ],
   "source": [
    "print('Load Glove embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=word_to_vec_map, d_model=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\2823419176.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['Comments']=new_df['Comments'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "new_df['Comments']=new_df['Comments'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
    "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
    "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\4036370795.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Comments without stopwords'] = data['Comments'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\4036370795.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_without_stopwords['clean_Comments']= data_without_stopwords['Comments without stopwords'].apply(lambda cw : remove_tags(cw))\n",
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\4036370795.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_without_stopwords['clean_Comments'] = data_without_stopwords['clean_Comments'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
      "C:\\Users\\VATSALA NEMA\\AppData\\Local\\Temp\\ipykernel_9732\\4036370795.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_without_stopwords['clean_Comments'] = data_without_stopwords['clean_Comments'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "  data['Comments without stopwords'] = data['Comments'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "  return data\n",
    "\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result\n",
    "    \n",
    "data_without_stopwords = remove_stopwords(new_df)\n",
    "data_without_stopwords['clean_Comments']= data_without_stopwords['Comments without stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "data_without_stopwords['clean_Comments'] = data_without_stopwords['clean_Comments'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = data_without_stopwords['clean_Comments']\n",
    "\n",
    "tokenizer.fit_on_texts(data_without_stopwords[\"clean_Comments\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comments_list = []\n",
    "for i in range(len(comments)):\n",
    "  try:\n",
    "    Comments_list.append(comments[i])\n",
    "  except KeyError:\n",
    "         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data_without_stopwords['label']\n",
    "label = label[:808021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_without_stopwords[['clean_Comments', 'label']]\n",
    "data.to_csv('data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train:  (800000, 2)\n",
      "Shape of y_train:  (800000, 1)\n",
      "Shape of x_test:   (4041, 2)\n",
      "Shape of y_test:   (4041, 1)\n",
      "Shape of x_val:   (4021, 2)\n",
      "Shape of y_val:   (4021, 1)\n"
     ]
    }
   ],
   "source": [
    "x = data_without_stopwords[['clean_Comments', 'label']]\n",
    "y = pd.DataFrame(new_df, columns = ['label']) \n",
    "\n",
    "# Split dataset to train and test set.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.005, random_state=42)\n",
    "\n",
    "# Split train dataset to train and validation set.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.005, random_state=42)\n",
    "\n",
    "print(\"Shape of x_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", Y_train.shape)\n",
    "print(\"Shape of x_test:  \", X_test.shape)\n",
    "print(\"Shape of y_test:  \", Y_test.shape)\n",
    "print(\"Shape of x_val:  \", X_val.shape)\n",
    "print(\"Shape of y_val:  \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 3)\n",
      "(4021, 3)\n",
      "(4041, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([X_train, Y_train], axis=1)\n",
    "print(train.shape)\n",
    "\n",
    "valid = pd.concat([X_val, Y_val], axis=1)\n",
    "print(valid.shape)\n",
    "\n",
    "test = pd.concat([X_test, Y_test], axis=1)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked RNN Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier_(torch.nn.Module):\n",
    "    def __init__(self, cell_type, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n",
    "        super(RNNClassifier_, self).__init__()\n",
    "        cells={\"LSTM\":nn.LSTM, \"GRU\": nn.GRU}\n",
    "\n",
    "        self.output_size=output_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.cell_type=cell_type\n",
    "        self.word_embeddings=torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight=torch.nn.Parameter(weights, requires_grad=False)\n",
    "\n",
    "        self.nn=cells[cell_type](\n",
    "            input_size=self.embedding_dim, \n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        self.label_layer=torch.nn.Linear(hidden_dim*2, output_size)\n",
    "\n",
    "        self.act=torch.nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            batch_size=x.size(0)\n",
    "            x=self.word_embeddings(x)\n",
    "            lstm_out, hidden=self.nn(x, hidden)\n",
    "            out=lstm_out.contiguous().view(-1, self.hidden_dim*2)\n",
    "            out=self.label_layer(out)\n",
    "            #Keep only the hidden representation of the last item of the sequence as the representative of the sample.\n",
    "            out = out.view(batch_size, -1, self.output_size)\n",
    "            out = out[:, -1, :]\n",
    "\n",
    "            out = self.act(out)\n",
    "            \n",
    "            return out, hidden\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight=next(self.parameters()).data\n",
    "\n",
    "            hidden=weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device)\n",
    "            \n",
    "            return hidden  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Stacked RNN  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size):\n",
    "  clip=5\n",
    "\n",
    "  total_total_train_epoch_loss = list()\n",
    "  total_train_epoch_acc = list()\n",
    "      \n",
    "  total_val_epoch_loss = list()\n",
    "  total_val_epoch_acc = list()\n",
    "      \n",
    "  steps = list()\n",
    "  \n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    #set the model in training phase\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    train_epoch_loss = list()\n",
    "    train_epoch_acc = list()\n",
    "    \n",
    "    val_epoch_loss = list()\n",
    "    val_epoch_acc = list()\n",
    "\n",
    "    steps = 0\n",
    "    \n",
    "    for idx, batch in enumerate((train_iter)):\n",
    "      if model.cell_type == 'LSTM':\n",
    "        h = tuple([e.data for e in h])\n",
    "      elif model.cell_type == 'GRU':\n",
    "        h.detach_()\n",
    "        h = h.detach()\n",
    "\n",
    "      text = batch.Text\n",
    "      target = batch.Label\n",
    "\n",
    "      target = target.type(torch.LongTensor)\n",
    "\n",
    "      text = text.to(device)\n",
    "      target = target.to(device)\n",
    "\n",
    "      #resets the gradients after every batch\n",
    "      optim.zero_grad()\n",
    "\n",
    "      if text.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "      prediction, h = model(text, h)\n",
    "\n",
    "      #compute the loss\n",
    "      loss_train = loss(prediction.squeeze(), target.float())\n",
    "\n",
    "      #backpropage the loss and compute the gradients\n",
    "      loss_train.backward(retain_graph=True)\n",
    "\n",
    "      #compute the binary accuracy\n",
    "      pred = torch.round(prediction.squeeze())\n",
    "      num_corrects = (pred == target).float().sum()\n",
    "      acc = 100.0 * num_corrects / len(batch)\n",
    "\n",
    "      #keep the loss and accuracies values for visualization\n",
    "      train_epoch_loss.append(loss_train.item())\n",
    "      train_epoch_acc.append(acc.item())\n",
    "      \n",
    "      #clip gradients\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "      \n",
    "      #update the weights\n",
    "      optim.step()\n",
    "\n",
    "      steps += 1\n",
    "\n",
    "      # if steps % 100 == 0:\n",
    "      #   print(f'Epoch: {num_epochs}, Idx: {idx+1}, Training Loss: {loss_train.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "      \n",
    "    print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n",
    "\n",
    "    #set the model in evaluating phase\n",
    "    model.eval()\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "\n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "      for idx, batch in enumerate((valid_iter)):\n",
    "        if model.cell_type == 'LSTM':\n",
    "          val_h = tuple([e.data for e in val_h])\n",
    "        elif model.cell_type == 'GRU':\n",
    "          val_h.detach_()\n",
    "          val_h = val_h.detach()\n",
    "\n",
    "        text = batch.Text\n",
    "        target = batch.Label\n",
    "        \n",
    "        target = target.type(torch.LongTensor)\n",
    "        \n",
    "        text = text.to(device)\n",
    "        target = target.to(device)\n",
    "            \n",
    "        if text.shape[0] != batch_size:\n",
    "          continue\n",
    "\n",
    "        prediction, val_h = model(text, val_h)\n",
    "\n",
    "        #compute loss\n",
    "        loss_val = loss(prediction.squeeze(), target.float())\n",
    "\n",
    "        #compute accuracy\n",
    "        pred = torch.round(prediction.squeeze())\n",
    "        num_corrects = (pred == target).float().sum()\n",
    "        acc = 100.0 * num_corrects / len(batch)\n",
    "\n",
    "        #keep the loss and accuracies values for visualization\n",
    "        val_epoch_loss.append(loss_val.item())\n",
    "        val_epoch_acc.append(acc.item())\n",
    "\n",
    "        # # if steps_ % 10 == 0:\n",
    "        # print(f'Epoch: {num_epochs}, Idx: {idx+1}, Validation Loss: {loss_train.item():.4f}, Validation Accuracy: {acc.item(): .2f}%')\n",
    "   \n",
    "        print(f'Validation Epoch: {epoch}, Validation Loss: {np.mean(val_epoch_loss):.4f}, Validation Accuracy: {np.mean(val_epoch_acc): .2f}%')\n",
    "                                \n",
    "        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n",
    "        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n",
    "\n",
    "        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n",
    "        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n",
    "      \n",
    "      return (total_train_epoch_loss, total_train_epoch_acc,\n",
    "              total_val_epoch_loss, total_val_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'BucketIterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VATSALA NEMA\\Documents\\SEM 7\\NLP 2022\\Project- Sarcasm detection in Soc med\\RoBERTa (1).ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y124sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m BATCH_SIZE\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y124sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_iter, valid_iter, test_iter \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mBucketIterator\u001b[39m.\u001b[39msplits(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y124sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     (train, valid, test), sort_key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mText),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y124sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mBATCH_SIZE, repeat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y124sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'BucketIterator'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=2048\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, valid, test), sort_key=lambda x: len(x.Text),\n",
    "    batch_size=BATCH_SIZE, repeat=False, shuffle=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define hyperparameters\n",
    "lr = 2e-3\n",
    "\n",
    "batch_size = 2048\n",
    "# batch_size = 64\n",
    "\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "# num_layers = 5\n",
    "\n",
    "hidden_size = 128\n",
    "# hidden_size = 32\n",
    "# hidden_size = 64\n",
    "\n",
    "embedding_length = 50\n",
    "\n",
    "cell_type='GRU'\n",
    "# cell_type='LSTM'\n",
    "\n",
    "#initialize the model\n",
    "model = RNNClassifier_(cell_type, vocab_size=808, output_size=output_size, embedding_dim=embedding_length, hidden_dim=hidden_size, n_layers=num_layers, weights=embedding_matrix)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available()\n",
    "                      else 'cpu')\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "#define optimizer and loss\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.BCELoss().to(device)\n",
    "# loss = nn.BCEWithLogitsLoss().to(device)\n",
    "# loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#train and evaluate the model\n",
    "train_loss, train_acc, val_loss, val_acc = train_model(model=model, train_iter=train_iter, val_iter=valid_iter, optim=optim, loss=loss, num_epochs=10, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer: RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 72\n",
    "TRAIN_BATCH_SIZE = 6\n",
    "VALID_BATCH_SIZE = 4\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df[1:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.Comments\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (808062, 6)\n",
      "TRAIN Dataset: (3998, 4)\n",
      "TEST Dataset: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments without stopwords</th>\n",
       "      <th>clean_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shouldve put</td>\n",
       "      <td>0</td>\n",
       "      <td>shouldve put</td>\n",
       "      <td>shouldve put</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realize talk nuclear deterrence right</td>\n",
       "      <td>1</td>\n",
       "      <td>realize talk nuclear deterrence right</td>\n",
       "      <td>realize talk nuclear deterrence right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extra cable come wireless headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>extra cable come wireless headphones</td>\n",
       "      <td>extra cable come wireless headphones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name</td>\n",
       "      <td>1</td>\n",
       "      <td>name</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv advance color settings color correction exp...</td>\n",
       "      <td>0</td>\n",
       "      <td>tv advance color settings color correction exp...</td>\n",
       "      <td>tv advance color settings color correction exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>better way would online bo uplink insertion</td>\n",
       "      <td>0</td>\n",
       "      <td>better way online bo uplink insertion</td>\n",
       "      <td>better way online bo uplink insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mean thorn shotty track rocket warlock</td>\n",
       "      <td>0</td>\n",
       "      <td>mean thorn shotty track rocket warlock</td>\n",
       "      <td>mean thorn shotty track rocket warlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>man idiot would design ship like</td>\n",
       "      <td>0</td>\n",
       "      <td>man idiot design ship like</td>\n",
       "      <td>man idiot design ship like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>take one know one hypothesis know net loss rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>take one know one hypothesis know net loss rea...</td>\n",
       "      <td>take one know one hypothesis know net loss rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>would totally buy gold failstorm</td>\n",
       "      <td>0</td>\n",
       "      <td>totally buy gold failstorm</td>\n",
       "      <td>totally buy gold failstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comments  label  \\\n",
       "0                                        shouldve put      0   \n",
       "1               realize talk nuclear deterrence right      1   \n",
       "2                extra cable come wireless headphones      1   \n",
       "3                                                name      1   \n",
       "4   tv advance color settings color correction exp...      0   \n",
       "..                                                ...    ...   \n",
       "95        better way would online bo uplink insertion      0   \n",
       "96             mean thorn shotty track rocket warlock      0   \n",
       "97                   man idiot would design ship like      0   \n",
       "98  take one know one hypothesis know net loss rea...      0   \n",
       "99                   would totally buy gold failstorm      0   \n",
       "\n",
       "                           Comments without stopwords  \\\n",
       "0                                        shouldve put   \n",
       "1               realize talk nuclear deterrence right   \n",
       "2                extra cable come wireless headphones   \n",
       "3                                                name   \n",
       "4   tv advance color settings color correction exp...   \n",
       "..                                                ...   \n",
       "95              better way online bo uplink insertion   \n",
       "96             mean thorn shotty track rocket warlock   \n",
       "97                         man idiot design ship like   \n",
       "98  take one know one hypothesis know net loss rea...   \n",
       "99                         totally buy gold failstorm   \n",
       "\n",
       "                                       clean_Comments  \n",
       "0                                        shouldve put  \n",
       "1               realize talk nuclear deterrence right  \n",
       "2                extra cable come wireless headphones  \n",
       "3                                                name  \n",
       "4   tv advance color settings color correction exp...  \n",
       "..                                                ...  \n",
       "95              better way online bo uplink insertion  \n",
       "96             mean thorn shotty track rocket warlock  \n",
       "97                         man idiot design ship like  \n",
       "98  take one know one hypothesis know net loss rea...  \n",
       "99                         totally buy gold failstorm  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:500]\n",
    "test_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%500==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 0.9915245175361633\n",
      "Training Accuracy per 500 steps: 66.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:18,  2.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VATSALA NEMA\\Documents\\SEM 7\\NLP 2022\\Project- Sarcasm detection in Soc med\\RoBERTa (1).ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32mc:\\Users\\VATSALA NEMA\\Documents\\SEM 7\\NLP 2022\\Project- Sarcasm detection in Soc med\\RoBERTa (1).ipynb Cell 63\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# # When using GPU\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y110sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    142\u001b[0m            grads,\n\u001b[0;32m    143\u001b[0m            exp_avgs,\n\u001b[0;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m            state_steps,\n\u001b[0;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\torch\\optim\\_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RobertaClass' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VATSALA NEMA\\Documents\\SEM 7\\NLP 2022\\Project- Sarcasm detection in Soc med\\RoBERTa (1).ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/VATSALA%20NEMA/Documents/SEM%207/NLP%202022/Project-%20Sarcasm%20detection%20in%20Soc%20med/RoBERTa%20%281%29.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict(new_df)\n",
      "File \u001b[1;32mc:\\Apps\\Conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RobertaClass' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "model.predict(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=808060 \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(32))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "de80e3fe07639b3e7fbeb7e635ff6e35cfe92ee04df37100b4cf21ce78aba9cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
